{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Example Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example has been adapted form the below blog post:\n",
    "\n",
    "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "For the mathematical explaination read my blog post and neural network fundamentals series here:\n",
    "\n",
    "https://samzee.net/2019/02/20/neural-networks-learning-the-basics-backpropagation/\n",
    "\n",
    "The standard process for building the neural network is as follows:\n",
    "1. Initialize Network\n",
    "2. Forward Propagate\n",
    "3. Back Propagate Error\n",
    "4. Train Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce example in blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialze_network(n_input, n_hidden, n_output):\n",
    "    network = []\n",
    "    hidden_layer = {'weights': [np.random.rand(n_input,n_hidden) , np.random.rand(n_hidden)]}\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = {'weights': [np.random.rand(n_hidden,n_output) , np.random.rand(n_output)]}\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "#defines sigmoid function\n",
    "def sigmoid(x):\n",
    "    s = 1/(1 + np.exp(-x))\n",
    "    return s\n",
    "#derivative of the loss function delta rule\n",
    "def dloss(target, output):\n",
    "    error = -(target - output)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the network\n",
    "random.seed(1)\n",
    "network = initialze_network(2, 2, 2)\n",
    "weights = [neuron['weights'] for neuron in network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing example\n",
    "x = np.array([[0.15,0.2],[0.25,0.3]])\n",
    "z = np.array([[0.4,0.45],[0.50,0.55]])\n",
    "inputs = np.array([0.05, 0.1])\n",
    "target = np.array([0.01, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update randomised inputs to fixed inputs\n",
    "weights[0][0] = x\n",
    "weights[1][0] = z\n",
    "weights[0][1] = np.array([0.35, 0.35]) #removed bias from the hidden layer\n",
    "weights[1][1] = np.array([0.6, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass\n",
    "def forward_pass(weights, inputs, bias):\n",
    "    a = np.dot(inputs, weights.T) + bias\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hidden = forward_pass(weights[0][0], inputs, weights[0][1])\n",
    "#activation\n",
    "a_hidden = sigmoid(a_hidden)\n",
    "#output\n",
    "outputs = forward_pass(weights[1][0], a_hidden, weights[1][1])\n",
    "#activation\n",
    "outputs = sigmoid(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the backward pass outputs\n",
    "def out_gradient(target, output, inputs):\n",
    "    gradient = (dloss(target, output)*output*(1- output))*inputs\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_gradient(target, output):\n",
    "    gradient = dloss(target, output)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = [out_gradient(target, outputs, item) for item in a_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08216704,  0.08266763],\n",
       "       [-0.02260254, -0.02274024]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output gradients\n",
    "gradients = np.vstack(grad).T\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_grad = bias_gradient(target, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hidden layer back prop\n",
    "def hid_gradient(target, output, inputs, a, weight):\n",
    "    k = np.sum((-(target - output)*output*(1 - output))*weight) #the first column of a matrix\n",
    "    g = a*(1- a)*inputs\n",
    "    return g*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00043857, 0.00049913],\n",
       "       [0.00087464, 0.00099543]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply\n",
    "w = weights[1][0].T\n",
    "h_gradients = []\n",
    "for item in w:\n",
    "    grad = hid_gradient(target, outputs, inputs, a_hidden, item)\n",
    "    h_gradients.append(grad)\n",
    "mygrad = np.vstack(h_gradients).T\n",
    "mygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the learning rate\n",
    "#update weights\n",
    "l = 0.5\n",
    "weights[0][0] = weights[0][0] - l*mygrad\n",
    "weights[1][0] = weights[1][0] - l*gradients\n",
    "weights[1][1] = weights[1][1] - 1*bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[0.14978072, 0.19975043],\n",
       "         [0.24956268, 0.29950229]]), array([0.35, 0.35])],\n",
       " [array([[0.35891648, 0.40866619],\n",
       "         [0.51130127, 0.56137012]]), array([-0.14136507,  0.81707153])]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialze_network(n_input, n_hidden, n_output):\n",
    "    network = []\n",
    "    hidden_layer = {'weights': [[np.random.rand(n_input,n_hidden)] , [0]]}\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = {'weights': [[np.random.rand(n_hidden,n_output)] , [random.random()]]}\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "#forward pass\n",
    "def forward_pass(weights, inputs, bias):\n",
    "    a = np.dot(inputs, weights) + bias\n",
    "    return a\n",
    "def out_gradient(target, output, inputs):\n",
    "    gradient = (dloss(target, output)*output*(1- output))*inputs\n",
    "    return gradient\n",
    "def bias_gradient(target, output):\n",
    "    gradient = dloss(target, output)\n",
    "    return gradient\n",
    "#one hidden layer back prop\n",
    "def hid_gradient(target, output, inputs, a, weight):\n",
    "    k = np.sum((-(target - output)*output*(1 - output))*weight) #the first column of a matrix\n",
    "    g = a*(1- a)*inputs\n",
    "    return g*k\n",
    "\n",
    "#loss function\n",
    "def lossfunction(target,output):\n",
    "    loss = (1/2)*(target - output)**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, inputs, target, l, n_epoch):\n",
    "    weights = [neuron['weights'] for neuron in network]\n",
    "    for epoch in range(n_epoch):\n",
    "        myloss = []\n",
    "        h_gradients = []\n",
    "        for i in range(len(inputs)):\n",
    "            a_hidden = forward_pass(weights[0][0], inputs[i], weights[0][1])\n",
    "            a_hidden = sigmoid(a_hidden) #apply activation\n",
    "            outputs = forward_pass(weights[1][0], a_hidden, weights[1][1])\n",
    "            outputs = sigmoid(outputs)\n",
    "            loss = lossfunction(target[i], outputs)\n",
    "            t_loss = np.sum(loss)\n",
    "            myloss.append(t_loss)\n",
    "            grad = [out_gradient(target[i], outputs, item) for item in a_hidden]\n",
    "            #print(\"this is output gradient {} for iteration {}\".format(grad, i))\n",
    "            gradients = np.vstack(grad).T     #output gradients\n",
    "            bias_grad = bias_gradient(target[i], outputs) \n",
    "            w = np.array(weights[1][0]).T\n",
    "            for item in w:\n",
    "                grad = hid_gradient(target[i], outputs, inputs[i], a_hidden[0], item)\n",
    "                h_gradients.append(grad)\n",
    "                mygrad = np.vstack(h_gradients).T\n",
    "                weights[0][0] = weights[0][0] - l*mygrad\n",
    "                weights[1][0] = weights[1][0] - l*gradients\n",
    "                weights[1][1] = weights[1][1] - l*bias_grad\n",
    "            final_loss = np.sum(myloss)\n",
    "            #print('>epoch={}, error={}'.format(n_epoch, final_loss))\n",
    "            return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training samples\n",
    "inputs = np.array([[2.7810836, 2.550537003], [1.465489372, 2.362125076], [396561688, 4.400293529], [1.38807019, 1.850220317],\n",
    "                  [3.06407232, 3.005305973],[7.627531214, 2.759262235],[5.332441248, 2.088626775],[6.922596716, 1.77106367],\n",
    "                  [8.675418651, 0.242068655], [7.673756466, 3.508563011]])\n",
    "target = np.array([[0], [0], [0], [0], [0], [1], [1], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from:\n",
    "\n",
    "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "An alternative implementation can also be accessed via this link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize network\n",
    "network = initialze_network(inputs.shape[1], 2, target.shape[1])\n",
    "weights = [neuron['weights'] for neuron in network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = [x for x in range(1, 50)]\n",
    "loss = []\n",
    "for n in n_epoch:\n",
    "    myloss = train_network(network, inputs, target, 0.5, n)\n",
    "    loss.append(myloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network works as we see the decrease in the loss function with each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(n_epoch[1:], loss[1:])\n",
    "#pyplot.plot(series_in, relu_out)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
