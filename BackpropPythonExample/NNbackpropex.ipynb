{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Example Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example has been adapted form the below blog post:\n",
    "\n",
    "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "For the mathematical explaination read my blog post and neural network fundamentals series here:\n",
    "\n",
    "https://samzee.net/2019/02/20/neural-networks-learning-the-basics-backpropagation/\n",
    "\n",
    "The standard process for building the neural network is as follows:\n",
    "1. Initialize Network\n",
    "2. Forward Propagate\n",
    "3. Back Propagate Error\n",
    "4. Train Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce example in blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialze_network(n_input, n_hidden, n_output):\n",
    "    network = []\n",
    "    hidden_layer = {'weights': [np.random.rand(n_input,n_hidden) , np.random.rand(n_hidden)]}\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = {'weights': [np.random.rand(n_hidden,n_output) , np.random.rand(n_output)]}\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "#defines sigmoid function\n",
    "def sigmoid(x):\n",
    "    s = 1/(1 + np.exp(-x))\n",
    "    return s\n",
    "#derivative of the loss function delta rule\n",
    "def dloss(target, output):\n",
    "    error = -(target - output)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the network\n",
    "random.seed(1)\n",
    "network = initialze_network(2, 2, 2)\n",
    "weights = [neuron['weights'] for neuron in network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing example\n",
    "x = np.array([[0.15,0.2],[0.25,0.3]])\n",
    "z = np.array([[0.4,0.45],[0.50,0.55]])\n",
    "inputs = np.array([0.05, 0.1])\n",
    "target = np.array([0.01, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update randomised inputs to fixed inputs\n",
    "weights[0][0] = x\n",
    "weights[1][0] = z\n",
    "weights[0][1] = np.array([0.35, 0.35]) #removed bias from the hidden layer\n",
    "weights[1][1] = np.array([0.6, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass\n",
    "def forward_pass(weights, inputs, bias):\n",
    "    a = np.dot(weights, inputs) + bias\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hidden = forward_pass(weights[0][0], inputs, weights[0][1])\n",
    "#activation\n",
    "a_hidden = sigmoid(a_hidden)\n",
    "#output\n",
    "outputs = forward_pass(weights[1][0], a_hidden, weights[1][1])\n",
    "#activation\n",
    "outputs = sigmoid(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the backward pass outputs\n",
    "def out_gradient(target, output, inputs):\n",
    "    gradient = (dloss(target, output)*output*(1- output))*inputs\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_gradient(target, output):\n",
    "    gradient = dloss(target, output)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = [out_gradient(target, outputs, item) for item in a_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08216704,  0.08266763],\n",
       "       [-0.02260254, -0.02274024]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output gradients\n",
    "gradients = np.vstack(grad).T\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_grad = bias_gradient(target, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hidden layer back prop\n",
    "def hid_gradient(target, output, inputs, a, weight):\n",
    "    k = np.sum((-(target - output)*output*(1 - output))*weight) #the first column of a matrix\n",
    "    g = a*(1- a)*inputs\n",
    "    return g*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00043857, 0.00049913],\n",
       "       [0.00087464, 0.00099543]])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply\n",
    "w = weights[1][0].T\n",
    "h_gradients = []\n",
    "for item in w:\n",
    "    grad = hid_gradient(target, outputs, inputs, a_hidden, item)\n",
    "    h_gradients.append(grad)\n",
    "mygrad = np.vstack(h_gradients).T\n",
    "mygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the learning rate\n",
    "#update weights\n",
    "l = 0.5\n",
    "weights[0][0] = weights[0][0] - l*mygrad\n",
    "weights[1][0] = weights[1][0] - l*gradients\n",
    "weights[1][1] = weights[1][1] - 1*bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[0.14978072, 0.19975043],\n",
       "         [0.24956268, 0.29950229]]), array([0.35, 0.35])],\n",
       " [array([[0.35891648, 0.40866619],\n",
       "         [0.51130127, 0.56137012]]), array([-0.14136507,  0.81707153])]]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialze_network(n_input, n_hidden, n_output):\n",
    "    network = []\n",
    "    hidden_layer = {'weights': [[np.random.rand(n_input,n_hidden)] , [0]]}\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = {'weights': [[np.random.rand(n_hidden,n_output)] , [random.random()]]}\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "#forward pass\n",
    "def forward_pass(weights, inputs, bias):\n",
    "    a = np.dot(weights, inputs) + bias\n",
    "    return a\n",
    "def out_gradient(target, output, inputs):\n",
    "    gradient = (dloss(target, output)*output*(1- output))*inputs\n",
    "    return gradient\n",
    "def bias_gradient(target, output):\n",
    "    gradient = dloss(target, output)\n",
    "    return gradient\n",
    "#one hidden layer back prop\n",
    "def hid_gradient(target, output, inputs, a, weight):\n",
    "    k = np.sum((-(target - output)*output*(1 - output))*weight) #the first column of a matrix\n",
    "    g = a*(1- a)*inputs\n",
    "    return g*k\n",
    "\n",
    "#loss function\n",
    "def lossfunction(target,output):\n",
    "    loss = (1/2)*(target - output)**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, inputs, target, l, n_epoch):\n",
    "    weights = [neuron['weights'] for neuron in network]\n",
    "    for epoch in range(n_epoch):\n",
    "        myloss = []\n",
    "        h_gradients = []\n",
    "        for i in range(len(inputs)):\n",
    "            a_hidden = forward_pass(weights[0][0][0], inputs[i], weights[0][1])\n",
    "            a_hidden = sigmoid(a_hidden) #apply activation\n",
    "            outputs = forward_pass(weights[1][0][0].T, a_hidden, weights[1][1])\n",
    "            outputs = sigmoid(outputs)\n",
    "            loss = lossfunction(target[i], outputs)\n",
    "            t_loss = np.sum(loss)\n",
    "            myloss.append(t_loss)\n",
    "            grad = [out_gradient(target[i], outputs, item) for item in a_hidden]\n",
    "            #print(\"this is output gradient {} for iteration {}\".format(grad, i))\n",
    "            gradients = np.vstack(grad).T     #output gradients\n",
    "            bias_grad = bias_gradient(target[i], outputs) \n",
    "            w = np.array(weights[1][0]).T\n",
    "            for item in w:\n",
    "                grad = hid_gradient(target[i], outputs, inputs[i], a_hidden[0], item)\n",
    "                h_gradients.append(grad)\n",
    "                mygrad = np.vstack(h_gradients).T\n",
    "                weights[0][0] = weights[0][0] - l*mygrad\n",
    "                weights[1][0] = weights[1][0] - l*gradients\n",
    "                weights[1][1] = weights[1][1] - l*bias_grad\n",
    "            final_loss = np.sum(myloss)\n",
    "            #print('>epoch={}, error={}'.format(n_epoch, final_loss))\n",
    "            return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training samples\n",
    "inputs = np.array([[2.7810836, 2.550537003], [1.465489372, 2.362125076], [396561688, 4.400293529], [1.38807019, 1.850220317],\n",
    "                  [3.06407232, 3.005305973],[7.627531214, 2.759262235],[5.332441248, 2.088626775],[6.922596716, 1.77106367],\n",
    "                  [8.675418651, 0.242068655], [7.673756466, 3.508563011]])\n",
    "target = np.array([[0], [0], [0], [0], [0], [1], [1], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from:\n",
    "\n",
    "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "An alternative implementation can also be accessed via this link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize network\n",
    "network = initialze_network(inputs.shape[1], 2, target.shape[1])\n",
    "weights = [neuron['weights'] for neuron in network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = [x for x in range(1, 50)]\n",
    "loss = []\n",
    "for n in n_epoch:\n",
    "    myloss = train_network(network, inputs, target, 0.5, n)\n",
    "    loss.append(myloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network works as we see the decrease in the loss function with each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(n_epoch[1:], loss[1:])\n",
    "#pyplot.plot(series_in, relu_out)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
