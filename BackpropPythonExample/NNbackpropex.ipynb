{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Example Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example has been adapted form the below blog post:\n",
    "\n",
    "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "For the mathematical explaination read my blog post and neural network fundamentals series here:\n",
    "\n",
    "https://samzee.net/2019/02/20/neural-networks-learning-the-basics-backpropagation/\n",
    "\n",
    "The standard process for building the neural network is as follows:\n",
    "1. Initialize Network\n",
    "2. Forward Propagate\n",
    "3. Back Propagate Error\n",
    "4. Train Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce example in blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialze_network(n_input, n_hidden, n_output):\n",
    "    network = []\n",
    "    hidden_layer = {'weights': [np.random.rand(n_input,n_hidden) , np.random.rand(n_hidden)]}\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = {'weights': [np.random.rand(n_hidden,n_output) , np.random.rand(n_output)]}\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "#defines sigmoid function\n",
    "def sigmoid(x):\n",
    "    s = 1/(1 + np.exp(-x))\n",
    "    return s\n",
    "#derivative of the loss function delta rule\n",
    "def dloss(target, output):\n",
    "    error = -(target - output)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the network\n",
    "random.seed(1)\n",
    "network = initialze_network(2, 2, 2)\n",
    "weights = [neuron['weights'] for neuron in network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing example\n",
    "x = np.array([[0.15,0.2],[0.25,0.3]])\n",
    "z = np.array([[0.4,0.45],[0.50,0.55]])\n",
    "inputs = np.array([0.05, 0.1])\n",
    "target = np.array([0.01, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update randomised inputs to fixed inputs\n",
    "weights[0][0] = x\n",
    "weights[1][0] = z\n",
    "weights[0][1] = np.array([0.35, 0.35]) #removed bias from the hidden layer\n",
    "weights[1][1] = np.array([0.6, 0.6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass\n",
    "def forward_pass(weights, inputs, bias):\n",
    "    a = np.dot(inputs, weights.T) + bias\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hidden = forward_pass(weights[0][0], inputs, weights[0][1])\n",
    "#activation\n",
    "a_hidden = sigmoid(a_hidden)\n",
    "#output\n",
    "outputs = forward_pass(weights[1][0], a_hidden, weights[1][1])\n",
    "#activation\n",
    "outputs = sigmoid(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the backward pass outputs\n",
    "def out_gradient(target, output, inputs):\n",
    "    gradient = (dloss(target, output)*output*(1- output))*inputs\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_gradient(target, output):\n",
    "    gradient = dloss(target, output)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = [out_gradient(target, outputs, item) for item in a_hidden]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08216704,  0.08266763],\n",
       "       [-0.02260254, -0.02274024]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output gradients\n",
    "gradients = np.vstack(grad).T\n",
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_grad = bias_gradient(target, outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hidden layer back prop\n",
    "def hid_gradient(target, output, inputs, a, weight):\n",
    "    k = np.sum((-(target - output)*output*(1 - output))*weight) #the first column of a matrix\n",
    "    g = a*(1- a)*inputs\n",
    "    return g*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00043857, 0.00049913],\n",
       "       [0.00087464, 0.00099543]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#apply\n",
    "w = weights[1][0].T\n",
    "h_gradients = []\n",
    "for item in w:\n",
    "    grad = hid_gradient(target, outputs, inputs, a_hidden, item)\n",
    "    h_gradients.append(grad)\n",
    "mygrad = np.vstack(h_gradients).T\n",
    "mygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the learning rate\n",
    "#update weights\n",
    "l = 0.5\n",
    "weights[0][0] = weights[0][0] - l*mygrad\n",
    "weights[1][0] = weights[1][0] - l*gradients\n",
    "weights[1][1] = weights[1][1] - 1*bias_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[0.14978072, 0.19975043],\n",
       "         [0.24956268, 0.29950229]]), array([0.35, 0.35])],\n",
       " [array([[0.35891648, 0.40866619],\n",
       "         [0.51130127, 0.56137012]]), array([-0.14136507,  0.81707153])]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialze_network(n_input, n_hidden, n_output):\n",
    "    network = []\n",
    "    hidden_layer = {'weights': [[np.random.rand(n_input,n_hidden)] , [0]]}\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = {'weights': [[np.random.rand(n_hidden,n_output)] , [random.random()]]}\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "#forward pass\n",
    "def forward_pass(weights, inputs, bias):\n",
    "    a = np.dot(inputs, weights.T) + bias\n",
    "    return a\n",
    "def out_gradient(target, output, inputs):\n",
    "    gradient = (dloss(target, output)*output*(1- output))*inputs\n",
    "    return gradient\n",
    "def bias_gradient(target, output):\n",
    "    gradient = dloss(target, output)\n",
    "    return gradient\n",
    "#one hidden layer back prop\n",
    "def hid_gradient(target, output, inputs, a, weight):\n",
    "    k = np.sum((-(target - output)*output*(1 - output))*weight) #the first column of a matrix\n",
    "    g = a*(1- a)*inputs\n",
    "    return g*k\n",
    "\n",
    "#loss function\n",
    "def lossfunction(target,output):\n",
    "    loss = (1/2)*(target - output)**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, inputs, target, l, n_epoch):\n",
    "    weights = [neuron['weights'] for neuron in network]\n",
    "    for epoch in range(n_epoch):\n",
    "        myloss = []\n",
    "        h_gradients = []\n",
    "        for i in range(len(inputs)):\n",
    "            a_hidden = forward_pass(weights[0][0][0], inputs[i], weights[0][1])\n",
    "            a_hidden = sigmoid(a_hidden) #apply activation\n",
    "            outputs = forward_pass(weights[1][0][0].T, a_hidden, weights[1][1])\n",
    "            outputs = sigmoid(outputs)\n",
    "            loss = lossfunction(target[i], outputs)\n",
    "            t_loss = np.sum(loss)\n",
    "            myloss.append(t_loss)\n",
    "            grad = [out_gradient(target[i], outputs, item) for item in a_hidden]\n",
    "            #print(\"this is output gradient {} for iteration {}\".format(grad, i))\n",
    "            gradients = np.vstack(grad).T     #output gradients\n",
    "            bias_grad = bias_gradient(target[i], outputs) \n",
    "            w = np.array(weights[1][0]).T\n",
    "            for item in w:\n",
    "                grad = hid_gradient(target[i], outputs, inputs[i], a_hidden[0], item)\n",
    "                h_gradients.append(grad)\n",
    "                mygrad = np.vstack(h_gradients).T\n",
    "                weights[0][0] = weights[0][0] - l*mygrad\n",
    "                weights[1][0] = weights[1][0] - l*gradients\n",
    "                weights[1][1] = weights[1][1] - l*bias_grad\n",
    "            final_loss = np.sum(myloss)\n",
    "            #print('>epoch={}, error={}'.format(n_epoch, final_loss))\n",
    "            return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training samples\n",
    "inputs = np.array([[2.7810836, 2.550537003], [1.465489372, 2.362125076], [396561688, 4.400293529], [1.38807019, 1.850220317],\n",
    "                  [3.06407232, 3.005305973],[7.627531214, 2.759262235],[5.332441248, 2.088626775],[6.922596716, 1.77106367],\n",
    "                  [8.675418651, 0.242068655], [7.673756466, 3.508563011]])\n",
    "target = np.array([[0], [0], [0], [0], [0], [1], [1], [1], [1], [1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from:\n",
    "\n",
    "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "An alternative implementation can also be accessed via this link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize network\n",
    "network = initialze_network(inputs.shape[1], 2, target.shape[1])\n",
    "weights = [neuron['weights'] for neuron in network]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = [x for x in range(1, 50)]\n",
    "loss = []\n",
    "for n in n_epoch:\n",
    "    myloss = train_network(network, inputs, target, 0.5, n)\n",
    "    loss.append(myloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network works as we see the decrease in the loss function with each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(n_epoch[1:], loss[1:])\n",
    "#pyplot.plot(series_in, relu_out)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
